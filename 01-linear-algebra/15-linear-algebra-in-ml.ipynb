{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Linear Algebra in Machine Learning\n",
    "---\n",
    "**Author:** Hamna Munir  \n",
    "**Repository:** Math-for-Machine-Learning  \n",
    "**Notebook:** 15-linear-algebra-in-ml.ipynb\n",
    "\n",
    "**Goal:** Understand how linear algebra concepts are used in real Machine Learning algorithms.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concept",
   "metadata": {},
   "source": [
    "## Concept\n",
    "Linear algebra provides the mathematical foundation for representing data, training models, and optimizing learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-explanation",
   "metadata": {},
   "source": [
    "## Mathematical Explanation\n",
    "Key concepts used in ML:\n",
    "\n",
    "- Vectors: represent data samples\n",
    "- Matrices: represent datasets and parameters\n",
    "- Dot products: measure similarity\n",
    "- Eigenvalues: dimensionality reduction\n",
    "- SVD: matrix factorization\n",
    "- Projections: least squares\n",
    "- Norms: regularization\n",
    "- Rank: information content\n",
    "\n",
    "Most ML models can be written as matrix equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-section",
   "metadata": {},
   "source": [
    "## Representing Data as Matrices\n",
    "Convert dataset into matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "data-matrix",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 3],\n",
       "       [3, 4],\n",
       "       [4, 5]])"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature matrix\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-regression-section",
   "metadata": {},
   "source": [
    "## Linear Regression Using Matrices\n",
    "Normal equation solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regression-code",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.5, 1. ])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "y = np.array([3, 5, 7, 9])\n",
    "\n",
    "# Add bias term\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Normal equation\n",
    "theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-section",
   "metadata": {},
   "source": [
    "## Similarity Using Dot Product\n",
    "Cosine similarity measures closeness of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "similarity-code",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9922778767136677"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 4, 6])\n",
    "\n",
    "cosine_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca-section",
   "metadata": {},
   "source": [
    "## PCA Using SVD\n",
    "Dimensionality reduction with matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pca-code",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2.12132034],\n",
       "       [ 0.        ],\n",
       "       [-2.12132034],\n",
       "       [ 0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Center data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "U, S, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "# First principal component\n",
    "X_reduced = U[:, :1] * S[:1]\n",
    "\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularization-section",
   "metadata": {},
   "source": [
    "## Regularization Using Norms\n",
    "Prevent overfitting with L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regularization-code",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.118033988749895"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "weights = np.array([0.5, 1.0])\n",
    "\n",
    "l2_norm = np.linalg.norm(weights)\n",
    "l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml-connection",
   "metadata": {},
   "source": [
    "## ML Connection\n",
    "Linear algebra powers:\n",
    "- Neural networks (matrix multiplications)\n",
    "- Regression models\n",
    "- PCA and clustering\n",
    "- Recommendation systems\n",
    "- NLP embeddings\n",
    "- Computer vision pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Data is stored as matrices\n",
    "- Models use matrix equations\n",
    "- SVD enables PCA\n",
    "- Norms control overfitting\n",
    "- Similarity uses dot products\n",
    "- Linear algebra is core to ML\n",
    "\n",
    "Developed by **Hamna Munir**\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
